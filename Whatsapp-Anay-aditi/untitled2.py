# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_WUoo97MfrJGMM-vjWzmPyft9KlHRyFO
"""



"""# Task
Analyze the emotional content of WhatsApp chat data from `/content/WhatsApp Chat with ECE A 5th semester.txt` and `/content/WhatsApp Chat with HackNx Community.txt` using an LSTM-based sentiment analysis model, and present the findings through a Streamlit application with interactive visualizations.

## Load and Parse WhatsApp Chat Data

### Subtask:
Load the provided WhatsApp chat text files (`/content/WhatsApp Chat with ECE A 5th semester.txt`, `/content/WhatsApp Chat with HackNx Community.txt`) and parse their content to extract individual messages, timestamps, and sender information. This data will be structured into a pandas DataFrame for further processing.

**Reasoning**:
The first step is to import the necessary libraries, `pandas` for data manipulation and `re` for regular expression operations, as specified in the instructions. Then, I will define a function to parse the WhatsApp chat files, handling the extraction of messages, timestamps, and sender information using regular expressions, and convert the parsed data into a pandas DataFrame.
"""

import pandas as pd
import re

def parse_whatsapp_chat(file_path):
    """
    Parses a WhatsApp chat file to extract messages, timestamps, and sender information.
    Returns a pandas DataFrame.
    """
    chat_data = []
    current_message = {"timestamp": None, "sender": None, "message": ""}

    # Regex pattern to match WhatsApp chat lines:
    # [DD/MM/YY, HH:MM] Sender: Message
    # The date format can vary, so a more flexible pattern might be needed if the format is not consistent.
    # For simplicity, assuming a common format like [DD/MM/YY, HH:MM]
    # Or [D/M/YY, H:MM] or [DD/M/YY, H:MM] etc.
    # Let's try to make it flexible for single or double digits for day, month, hour, minute

    # Pattern to capture timestamp, sender, and message content
    # Updated regex to handle common WhatsApp date/time formats and sender names, including potential newlines within messages.
    # It now specifically looks for the pattern '[date, time] Sender: message' at the beginning of a line.
    message_pattern = re.compile(r'^\u200e?(\[\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2,3}(?: AM| PM)?\]) ([^:]+): (.*)')


    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # Check for system messages or encrypted messages that should be ignored
            if "Messages to this group are now end-to-end encrypted" in line or "You created group" in line or "changed the subject from" in line or "added" in line or "left" in line:
                continue

            match = message_pattern.match(line)
            if match:
                # If a new message starts, save the previous one if it exists
                if current_message["timestamp"] is not None:
                    chat_data.append(current_message)

                # Start a new message
                timestamp_str = match.group(1).strip('\u200e[]')
                sender = match.group(2)
                message = match.group(3)

                # Try to parse different date formats
                # WhatsApp uses various date formats based on locale and app version
                # Common formats: DD/MM/YY, DD/MM/YYYY, D/M/YY, D/M/YYYY
                # And time: HH:MM or H:MM

                # Attempt to parse timestamp. This needs to be robust.
                # Let's clean the timestamp string and try a few common formats

                try:
                    # Try YYYY format first if possible, then YY
                    # Assume the chat files are from the 21st century for now (2000s)
                    current_message = {
                        "timestamp": pd.to_datetime(timestamp_str, format='%d/%m/%y, %H:%M', errors='coerce') or \
                                     pd.to_datetime(timestamp_str, format='%d/%m/%Y, %H:%M', errors='coerce') or \
                                     pd.to_datetime(timestamp_str, format='%d/%m/%y, %I:%M %p', errors='coerce') or \
                                     pd.to_datetime(timestamp_str, format='%d/%m/%Y, %I:%M %p', errors='coerce'),
                        "sender": sender,
                        "message": message
                    }
                except Exception as e:
                    # Fallback for parsing errors, just store as string
                    current_message = {
                        "timestamp": timestamp_str,
                        "sender": sender,
                        "message": message
                    }

            else:
                # If the line doesn't match the pattern, it's likely a continuation of the previous message
                if current_message["timestamp"] is not None:
                    current_message["message"] += "\n" + line

    # Add the last message to the list
    if current_message["timestamp"] is not None:
        chat_data.append(current_message)

    df = pd.DataFrame(chat_data)

    # Ensure timestamp is datetime if it wasn't parsed successfully in the loop
    # This handles cases where pd.to_datetime in the loop returned NaT and we stored the string
    if not df.empty and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# File paths
file_path_ece = '/content/WhatsApp Chat with ECE A 5th semester.txt'
file_path_hacknx = '/content/WhatsApp Chat with HackNx Community.txt'

# Parse each file
df_ece = parse_whatsapp_chat(file_path_ece)
df_hacknx = parse_whatsapp_chat(file_path_hacknx)

# Concatenate the DataFrames
df_combined = pd.concat([df_ece, df_hacknx], ignore_index=True)

# Display the first few rows and information of the combined DataFrame
print("First 5 rows of the combined DataFrame:")
print(df_combined.head())
print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The previous attempt to parse the chat files resulted in empty DataFrames, indicating an issue with the regular expression or the timestamp parsing logic. I will refine the regular expression to be more robust in matching various WhatsApp message formats and improve the timestamp conversion to handle different date/time formats more reliably. The `pd.to_datetime` calls were using `or` which might not work as intended with `NaT` values. Instead, I'll use a loop to try multiple formats sequentially and ensure the correct parsing of timestamps.


"""

import pandas as pd
import re

def parse_whatsapp_chat(file_path):
    """
    Parses a WhatsApp chat file to extract messages, timestamps, and sender information.
    Returns a pandas DataFrame.
    """
    chat_data = []
    current_message = {"timestamp": None, "sender": None, "message": ""}

    # Updated regex pattern to handle common WhatsApp chat formats more robustly.
    # It captures: Optional invisible character, timestamp, sender, and message.
    # Timestamp format: [D/M/YY, H:MM] or [DD/MM/YYYY, HH:MM AM/PM]
    # Sender: Any characters until the first colon.
    # Message: Rest of the line.
    message_pattern = re.compile(r'^\u200e?\[(\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?: [AP]M)?)\] ([^:]+): (.*)')

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # Check for system messages or encrypted messages that should be ignored
            if "Messages to this group are now end-to-end encrypted" in line or \
               "You created group" in line or \
               "changed the subject from" in line or \
               "added" in line or \
               "left" in line or \
               "deleted" in line or \
               "changed the group" in line:
                continue

            match = message_pattern.match(line)
            if match:
                # If a new message starts, save the previous one if it exists
                if current_message["timestamp"] is not None:
                    chat_data.append(current_message)

                # Start a new message
                timestamp_str = match.group(1).strip()
                sender = match.group(2).strip()
                message = match.group(3).strip()

                # Robust timestamp parsing using multiple formats
                parsed_timestamp = pd.NaT
                timestamp_formats = [
                    '%d/%m/%y, %H:%M',    # e.g., 01/01/20, 14:30
                    '%d/%m/%Y, %H:%M',    # e.g., 01/01/2020, 14:30
                    '%d/%m/%y, %I:%M %p', # e.g., 01/01/20, 02:30 PM
                    '%d/%m/%Y, %I:%M %p', # e.g., 01/01/2020, 02:30 PM
                    '%m/%d/%y, %H:%M',    # e.g., 1/1/20, 14:30 (US format)
                    '%m/%d/%Y, %H:%M',    # e.g., 1/1/2020, 14:30
                    '%m/%d/%y, %I:%M %p', # e.g., 1/1/20, 02:30 PM
                    '%m/%d/%Y, %I:%M %p'  # e.g., 1/1/2020, 02:30 PM
                ]

                for fmt in timestamp_formats:
                    try:
                        parsed_timestamp = pd.to_datetime(timestamp_str, format=fmt)
                        if pd.notna(parsed_timestamp):
                            break # Found a valid format, stop trying
                    except ValueError:
                        continue # Try the next format

                current_message = {
                    "timestamp": parsed_timestamp,
                    "sender": sender,
                    "message": message
                }

            else:
                # If the line doesn't match the pattern, it's likely a continuation of the previous message
                if current_message["timestamp"] is not None:
                    current_message["message"] += "\n" + line

    # Add the last message to the list after the loop finishes
    if current_message["timestamp"] is not None:
        chat_data.append(current_message)

    df = pd.DataFrame(chat_data)

    # Ensure timestamp column is of datetime type if not already, coercing errors
    if not df.empty and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# File paths
file_path_ece = '/content/WhatsApp Chat with ECE A 5th semester.txt'
file_path_hacknx = '/content/WhatsApp Chat with HackNx Community.txt'

# Parse each file
df_ece = parse_whatsapp_chat(file_path_ece)
df_hacknx = parse_whatsapp_chat(file_path_hacknx)

# Concatenate the DataFrames
df_combined = pd.concat([df_ece, df_hacknx], ignore_index=True)

# Display the first few rows and information of the combined DataFrame
print("First 5 rows of the combined DataFrame:")
print(df_combined.head())
print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The parsing function still produces empty dataframes, indicating that the regular expression for matching message lines is likely still incorrect or not comprehensive enough for the actual chat file format. I need to further refine the regex to accurately capture the message structure, specifically checking for common variations in WhatsApp chat exports. I will also add print statements to inspect the number of parsed messages from each file, which can help debug if the regex is the main problem.


"""

import pandas as pd
import re

def parse_whatsapp_chat(file_path):
    """
    Parses a WhatsApp chat file to extract messages, timestamps, and sender information.
    Returns a pandas DataFrame.
    """
    chat_data = []
    current_message = {"timestamp": None, "sender": None, "message": ""}

    # Further refined regex pattern for common WhatsApp chat formats.
    # It needs to be very robust to date/time format variations, presence of seconds, and sender format.
    # Common formats: '[DD/MM/YY, HH:MM]' or '[DD/MM/YYYY, HH:MM:SS AM/PM]' followed by ' Sender: Message'
    # The first group captures the timestamp string.
    # The second group captures the sender (anything before the first colon, excluding the timestamp part).
    # The third group captures the message content.
    # Added optional seconds (?: :\d{2})? and made AM/PM case-insensitive.
    # Added \u200e? at the start for the invisible character sometimes present.
    message_pattern = re.compile(r'^\u200e?\[(\d{1,2}\/\d{1,2}\/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?(?: [APap][Mm])?)\] ([^:]+): (.*)')

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # Check for system messages or encrypted messages that should be ignored
            # Expanded the list of system messages to ignore.
            if "Messages to this group are now end-to-end encrypted" in line or \
               "You created group" in line or \
               "changed the subject from" in line or \
               "added" in line or \
               "left" in line or \
               "deleted" in line or \
               "changed the group" in line or \
               "changed this group's icon" in line or \
               "You were added" in line or \
               "You joined" in line or \
               "started a call" in line or \
               "missed voice call" in line or \
               "missed video call" in line:
                continue

            match = message_pattern.match(line)
            if match:
                # If a new message starts, save the previous one if it exists
                if current_message["timestamp"] is not None:
                    chat_data.append(current_message)

                # Start a new message
                timestamp_str = match.group(1).strip() # group 1 is the timestamp string without brackets
                sender = match.group(2).strip()
                message = match.group(3).strip()

                # Robust timestamp parsing using multiple formats
                parsed_timestamp = pd.NaT
                timestamp_formats = [
                    '%d/%m/%y, %H:%M',    # e.g., 01/01/20, 14:30
                    '%d/%m/%Y, %H:%M',    # e.g., 01/01/2020, 14:30
                    '%d/%m/%y, %I:%M %p', # e.g., 01/01/20, 02:30 PM
                    '%d/%m/%Y, %I:%M %p', # e.g., 01/01/2020, 02:30 PM
                    '%m/%d/%y, %H:%M',    # e.g., 1/1/20, 14:30 (US format)
                    '%m/%d/%Y, %H:%M',    # e.g., 1/1/2020, 14:30
                    '%m/%d/%y, %I:%M %p', # e.g., 1/1/20, 02:30 PM
                    '%m/%d/%Y, %I:%M %p', # e.g., 1/1/2020, 02:30 PM
                    # Added formats for seconds
                    '%d/%m/%y, %H:%M:%S',    # e.g., 01/01/20, 14:30:05
                    '%d/%m/%Y, %H:%M:%S',    # e.g., 01/01/2020, 14:30:05
                    '%d/%m/%y, %I:%M:%S %p', # e.g., 01/01/20, 02:30:05 PM
                    '%d/%m/%Y, %I:%M:%S %p', # e.g., 01/01/2020, 02:30:05 PM
                    '%m/%d/%y, %H:%M:%S',    # e.g., 1/1/20, 14:30:05 (US format)
                    '%m/%d/%Y, %H:%M:%S',    # e.g., 1/1/2020, 14:30:05
                    '%m/%d/%y, %I:%M:%S %p', # e.g., 1/1/20, 02:30:05 PM
                    '%m/%d/%Y, %I:%M:%S %p'  # e.g., 1/1/2020, 02:30:05 PM
                ]

                for fmt in timestamp_formats:
                    try:
                        parsed_timestamp = pd.to_datetime(timestamp_str, format=fmt)
                        if pd.notna(parsed_timestamp):
                            break # Found a valid format, stop trying
                    except ValueError:
                        continue # Try the next format

                current_message = {
                    "timestamp": parsed_timestamp,
                    "sender": sender,
                    "message": message
                }

            else:
                # If the line doesn't match the pattern, it's likely a continuation of the previous message
                # This also covers media messages which might appear as just a line e.g., "<Media omitted>"
                if current_message["timestamp"] is not None:
                    # Check if the line is not a system message (already handled above) and not a common media placeholder
                    if not (line.strip() == "<Media omitted>" or line.strip() == "You deleted this message."):
                        current_message["message"] += "\n" + line

    # Add the last message to the list after the loop finishes
    if current_message["timestamp"] is not None:
        chat_data.append(current_message)

    df = pd.DataFrame(chat_data)

    # Ensure timestamp column is of datetime type if not already, coercing errors
    if not df.empty and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# File paths
file_path_ece = '/content/WhatsApp Chat with ECE A 5th semester.txt'
file_path_hacknx = '/content/WhatsApp Chat with HackNx Community.txt'

# Parse each file
df_ece = parse_whatsapp_chat(file_path_ece)
df_hacknx = parse_whatsapp_chat(file_path_hacknx)

# Concatenate the DataFrames
df_combined = pd.concat([df_ece, df_hacknx], ignore_index=True)

# Display the first few rows and information of the combined DataFrame
print("First 5 rows of the combined DataFrame:")
print(df_combined.head())
print("\nInformation about the combined DataFrame:")
df_combined.info()

print(f"\nDataFrame ECE shape: {df_ece.shape}")
print(f"DataFrame HackNx shape: {df_hacknx.shape}")
print(f"DataFrame Combined shape: {df_combined.shape}")

"""**Reasoning**:
Since the previous attempts to parse the chat files have consistently resulted in empty DataFrames, the most likely issue is the regular expression not correctly matching the format of the actual WhatsApp chat messages. To diagnose this, I need to inspect the raw content of the chat files directly. I will read a few lines from each file to understand their exact structure and then refine the regex pattern accordingly. Additionally, I will add a print statement inside the `parse_whatsapp_chat` function to show which lines are being processed and whether they match the regex, which will help in debugging.


"""

import pandas as pd
import re

def parse_whatsapp_chat(file_path):
    """
    Parses a WhatsApp chat file to extract messages, timestamps, and sender information.
    Returns a pandas DataFrame.
    """
    chat_data = []
    current_message = {"timestamp": None, "sender": None, "message": ""}

    # Debugging: Print a few lines from the file to understand the format
    print(f"\n--- Inspecting first 10 lines of {file_path} ---")
    with open(file_path, 'r', encoding='utf-8') as f_debug:
        for i, line_debug in enumerate(f_debug):
            print(f"Line {i+1}: {line_debug.strip()}")
            if i >= 9: # Print first 10 lines
                break
    print("--------------------------------------------------")

    # Further refined regex pattern for common WhatsApp chat formats.
    # It needs to be very robust to date/time format variations, presence of seconds, and sender format.
    # Common formats: '[DD/MM/YY, HH:MM]' or '[DD/MM/YYYY, HH:MM:SS AM/PM]' followed by ' Sender: Message'
    # The first group captures the timestamp string.
    # The second group captures the sender (anything before the first colon, excluding the timestamp part).
    # The third group captures the message content.
    # Added optional seconds (?: :\d{2})? and made AM/PM case-insensitive.
    # Added \u200e? at the start for the invisible character sometimes present.
    # The previous regex `[APap][Mm])?` might be too restrictive, let's try `[AaPp][Mm])?`
    # Also, some WhatsApp exports don't have the leading invisible character or even the brackets around date/time for some older formats or specific exports.
    # Let's try to match a more generic pattern:
    # Day/Month/Year, Hour:Minute (optional :Second) (optional AM/PM)
    # The date format might also be M/D/YY or M/D/YYYY. Let's make the day/month flexible.
    # The sender name can contain spaces and various characters.
    # The colon after sender might be followed by a space or not.
    message_pattern = re.compile(r'^â€Ž?(\[?\d{1,2}\/\d{1,2}\/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?(?: [APap][Mm])?\]?) ([^:]+): (.*)')

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # Check for system messages or encrypted messages that should be ignored
            if "Messages to this group are now end-to-end encrypted" in line or \
               "You created group" in line or \
               "changed the subject from" in line or \
               "added" in line or \
               "left" in line or \
               "deleted" in line or \
               "changed the group" in line or \
               "changed this group's icon" in line or \
               "You were added" in line or \
               "You joined" in line or \
               "started a call" in line or \
               "missed voice call" in line or \
               "missed video call" in line:
                continue

            match = message_pattern.match(line)
            if match:
                # Debugging: indicate a match was found
                # print(f"MATCHED: {line[:100]}...")
                if current_message["timestamp"] is not None:
                    chat_data.append(current_message)

                timestamp_str = match.group(1).strip().strip('[]') # Remove potential brackets here
                sender = match.group(2).strip()
                message = match.group(3).strip()

                parsed_timestamp = pd.NaT
                timestamp_formats = [
                    '%d/%m/%y, %H:%M',    # e.g., 01/01/20, 14:30
                    '%d/%m/%Y, %H:%M',    # e.g., 01/01/2020, 14:30
                    '%d/%m/%y, %I:%M %p', # e.g., 01/01/20, 02:30 PM
                    '%d/%m/%Y, %I:%M %p', # e.g., 01/01/2020, 02:30 PM
                    '%m/%d/%y, %H:%M',    # e.g., 1/1/20, 14:30 (US format)
                    '%m/%d/%Y, %H:%M',    # e.g., 1/1/2020, 14:30
                    '%m/%d/%y, %I:%M %p', # e.g., 1/1/20, 02:30 PM
                    '%m/%d/%Y, %I:%M %p', # e.g., 1/1/2020, 02:30 PM
                    '%d/%m/%y, %H:%M:%S',    # e.g., 01/01/20, 14:30:05
                    '%d/%m/%Y, %H:%M:%S',    # e.g., 01/01/2020, 14:30:05
                    '%d/%m/%y, %I:%M:%S %p', # e.g., 01/01/20, 02:30:05 PM
                    '%d/%m/%Y, %I:%M:%S %p', # e.g., 01/01/2020, 02:30:05 PM
                    '%m/%d/%y, %H:%M:%S',    # e.g., 1/1/20, 14:30:05 (US format)
                    '%m/%d/%Y, %H:%M:%S',    # e.g., 1/1/2020, 14:30:05
                    '%m/%d/%y, %I:%M:%S %p', # e.g., 1/1/20, 02:30:05 PM
                    '%m/%d/%Y, %I:%M:%S %p'  # e.g., 1/1/2020, 02:30:05 PM
                ]

                for fmt in timestamp_formats:
                    try:
                        parsed_timestamp = pd.to_datetime(timestamp_str, format=fmt)
                        if pd.notna(parsed_timestamp):
                            break # Found a valid format, stop trying
                    except ValueError:
                        continue # Try the next format

                current_message = {
                    "timestamp": parsed_timestamp,
                    "sender": sender,
                    "message": message
                }

            else:
                # Debugging: indicate no match for the line
                # print(f"NO MATCH: {line[:100]}...")
                if current_message["timestamp"] is not None:
                    if not (line.strip() == "<Media omitted>" or line.strip() == "You deleted this message."):
                        current_message["message"] += "\n" + line

    if current_message["timestamp"] is not None:
        chat_data.append(current_message)

    df = pd.DataFrame(chat_data)

    if not df.empty and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# File paths
file_path_ece = '/content/WhatsApp Chat with ECE A 5th semester.txt'
file_path_hacknx = '/content/WhatsApp Chat with HackNx Community.txt'

# Parse each file
df_ece = parse_whatsapp_chat(file_path_ece)
df_hacknx = parse_whatsapp_chat(file_path_hacknx)

# Concatenate the DataFrames
df_combined = pd.concat([df_ece, df_hacknx], ignore_index=True)

# Display the first few rows and information of the combined DataFrame
print("First 5 rows of the combined DataFrame:")
print(df_combined.head())
print("\nInformation about the combined DataFrame:")
df_combined.info()

print(f"\nDataFrame ECE shape: {df_ece.shape}")
print(f"DataFrame HackNx shape: {df_hacknx.shape}")
print(f"DataFrame Combined shape: {df_combined.shape}")

"""**Reasoning**:
The previous attempts failed because the regular expression did not correctly match the actual format of the WhatsApp chat messages, as revealed by inspecting the raw file content. The timestamp does not have brackets and the separator between timestamp and sender is ' - ' rather than a space or bracket. Also, the AM/PM part includes a narrow no-break space (`\u202f`). I will update the regular expression and timestamp parsing formats to reflect these findings, including month-day-year formats, and re-execute the parsing.


"""

import pandas as pd
import re

def parse_whatsapp_chat(file_path):
    """
    Parses a WhatsApp chat file to extract messages, timestamps, and sender information.
    Returns a pandas DataFrame.
    """
    chat_data = []
    current_message = {"timestamp": None, "sender": None, "message": ""}

    # Updated regex pattern based on inspected file content:
    # Timestamp format: M/D/YY, H:MM AM/PM (with potential narrow no-break space U+202F before AM/PM)
    # Separator between timestamp and sender: " - "
    # Sender name: anything until the next colon
    # Message: rest of the line after sender's colon and space
    message_pattern = re.compile(r'^(?P<timestamp>\d{1,2}/\d{1,2}/\d{2,4}, \d{1,2}:\d{2}(?::\d{2})?(?:\u202f?[APap][Mm])?) - (?P<sender>[^:]+): (?P<message>.*)')

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # Check for system messages or encrypted messages that should be ignored
            if "Messages to this group are now end-to-end encrypted" in line or \
               "You created group" in line or \
               "changed the subject from" in line or \
               "added" in line or \
               "left" in line or \
               "deleted" in line or \
               "changed the group" in line or \
               "changed this group's icon" in line or \
               "You were added" in line or \
               "You joined" in line or \
               "started a call" in line or \
               "missed voice call" in line or \
               "missed video call" in line or \
               "was added" in line: # Added "was added" for ~ Narendra was added type messages
                continue

            match = message_pattern.match(line)
            if match:
                # If a new message starts, save the previous one if it exists
                if current_message["timestamp"] is not None:
                    chat_data.append(current_message)

                # Start a new message
                timestamp_str = match.group("timestamp").strip()
                sender = match.group("sender").strip()
                message = match.group("message").strip()

                # Robust timestamp parsing using multiple formats
                parsed_timestamp = pd.NaT
                timestamp_formats = [
                    '%m/%d/%y, %H:%M',
                    '%m/%d/%Y, %H:%M',
                    '%m/%d/%y, %I:%M %p',
                    '%m/%d/%Y, %I:%M %p',
                    '%m/%d/%y, %H:%M:%S',
                    '%m/%d/%Y, %H:%M:%S',
                    '%m/%d/%y, %I:%M:%S %p',
                    '%m/%d/%Y, %I:%M:%S %p',
                    # Add formats for the narrow no-break space U+202F if pd.to_datetime doesn't handle it with %p
                    '%m/%d/%y, %I:%M\u202f%p',
                    '%m/%d/%Y, %I:%M\u202f%p',
                    '%m/%d/%y, %I:%M:%S\u202f%p',
                    '%m/%d/%Y, %I:%M:%S\u202f%p'
                ]

                for fmt in timestamp_formats:
                    try:
                        parsed_timestamp = pd.to_datetime(timestamp_str, format=fmt)
                        if pd.notna(parsed_timestamp):
                            break # Found a valid format, stop trying
                    except ValueError:
                        continue # Try the next format

                current_message = {
                    "timestamp": parsed_timestamp,
                    "sender": sender,
                    "message": message
                }

            else:
                # If the line doesn't match the pattern, it's likely a continuation of the previous message
                if current_message["timestamp"] is not None:
                    if not (line.strip() == "<Media omitted>" or line.strip() == "You deleted this message."):
                        current_message["message"] += "\n" + line

    # Add the last message to the list after the loop finishes
    if current_message["timestamp"] is not None:
        chat_data.append(current_message)

    df = pd.DataFrame(chat_data)

    # Ensure timestamp column is of datetime type if not already, coercing errors
    if not df.empty and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# File paths
file_path_ece = '/content/WhatsApp Chat with ECE A 5th semester.txt'
file_path_hacknx = '/content/WhatsApp Chat with HackNx Community.txt'

# Parse each file
df_ece = parse_whatsapp_chat(file_path_ece)
df_hacknx = parse_whatsapp_chat(file_path_hacknx)

# Concatenate the DataFrames
df_combined = pd.concat([df_ece, df_hacknx], ignore_index=True)

# Display the first few rows and information of the combined DataFrame
print("First 5 rows of the combined DataFrame:")
print(df_combined.head())
print("\nInformation about the combined DataFrame:")
df_combined.info()

print(f"\nDataFrame ECE shape: {df_ece.shape}")
print(f"DataFrame HackNx shape: {df_hacknx.shape}")
print(f"DataFrame Combined shape: {df_combined.shape}")

"""## Preprocess Text for Sentiment Analysis

### Subtask:
Clean the extracted chat messages by removing unnecessary elements (e.g., media omitted messages, links, non-alphanumeric characters, multiple spaces). Convert text to lowercase and perform tokenization.

**Reasoning**:
The subtask requires preprocessing the text data. First, I need to define a Python function `preprocess_text` to clean the messages by converting them to lowercase, removing URLs, specific placeholders, non-alphanumeric characters, and extra spaces. Then, I will apply this function to the 'message' column of `df_combined` to create a new 'clean_message' column.
"""

import re
import nltk

def preprocess_text(text):
    """
    Cleans a given text message by performing several preprocessing steps.
    """
    text = str(text).lower() # Convert to string and lowercase

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove specific placeholders
    text = re.sub(r'<media omitted>|you deleted this message\.', '', text)

    # Remove non-alphanumeric characters and replace with a single space
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)

    # Replace multiple spaces with a single space
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Apply the preprocessing function to the 'message' column
df_combined['clean_message'] = df_combined['message'].apply(preprocess_text)

# Display the first 5 rows with the new column
print("First 5 rows of df_combined with 'clean_message':")
print(df_combined[['message', 'clean_message']].head())

"""**Reasoning**:
I need to ensure the 'punkt' NLTK tokenizer is downloaded, then apply `nltk.word_tokenize` to the `clean_message` column to create a new `tokens` column, and finally display the updated DataFrame's head and info as requested by the subtask.


"""

import nltk
from nltk.tokenize import word_tokenize

# Download the 'punkt' tokenizer if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')

# Apply the word_tokenize function to the 'clean_message' column
df_combined['tokens'] = df_combined['clean_message'].apply(word_tokenize)

# Display the first 5 rows with the new columns
print("First 5 rows of df_combined with 'message', 'clean_message', and 'tokens':")
print(df_combined[['message', 'clean_message', 'tokens']].head())

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The previous code failed because the `except` block was catching `nltk.downloader.DownloadError`, but `nltk.data.find` raises a `LookupError` when the resource is not found. I will correct the exception type to `LookupError` to properly handle the case where the 'punkt' tokenizer needs to be downloaded.


"""

import nltk
from nltk.tokenize import word_tokenize

# Download the 'punkt' tokenizer if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Apply the word_tokenize function to the 'clean_message' column
df_combined['tokens'] = df_combined['clean_message'].apply(word_tokenize)

# Display the first 5 rows with the new columns
print("First 5 rows of df_combined with 'message', 'clean_message', and 'tokens':")
print(df_combined[['message', 'clean_message', 'tokens']].head())

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The previous code failed because, although `punkt` was downloaded, `nltk.word_tokenize` implicitly requires the `punkt_tab` resource for sentence tokenization, which was not found. I will add an explicit download for `punkt_tab` to resolve this dependency.


"""

import nltk
from nltk.tokenize import word_tokenize

# Download the 'punkt' tokenizer if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Download 'punkt_tab' as it's specifically mentioned in the error message
try:
    # This check for 'punkt_tab' isn't as direct as 'punkt', so we'll rely on the download mechanism.
    # However, to be consistent with the try-except pattern, we'll try to find a sub-resource
    # that would implicitly load 'punkt_tab' if it existed, or just assume the download is needed.
    # A safer approach is just to call download and let NLTK handle if it's already there.
    nltk.data.find('tokenizers/punkt_tab/english') # Try finding a specific part of punkt_tab
except LookupError:
    nltk.download('punkt_tab')

# Apply the word_tokenize function to the 'clean_message' column
df_combined['tokens'] = df_combined['clean_message'].apply(word_tokenize)

# Display the first 5 rows with the new columns
print("First 5 rows of df_combined with 'message', 'clean_message', and 'tokens':")
print(df_combined[['message', 'clean_message', 'tokens']].head())

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The previous steps successfully cleaned and tokenized the chat messages. The next logical step in text preprocessing for sentiment analysis is to remove common stopwords, which are words that do not carry significant emotional content and can add noise to the model. I will define a function to remove stopwords using NLTK's English stopwords list and apply it to the 'tokens' column.


"""

import nltk
from nltk.corpus import stopwords

# Download the 'stopwords' corpus if not already downloaded
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# Get English stopwords
stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
    """
    Removes stopwords from a list of tokens.
    """
    return [word for word in tokens if word not in stop_words]

# Apply the remove_stopwords function to the 'tokens' column
df_combined['filtered_tokens'] = df_combined['tokens'].apply(remove_stopwords)

# Display the first 5 rows with the new column
print("First 5 rows of df_combined with 'tokens' and 'filtered_tokens':")
print(df_combined[['tokens', 'filtered_tokens']].head())

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
After tokenization and stopword removal, the next logical step in text preprocessing for sentiment analysis is lemmatization. This process reduces words to their base or dictionary form, which helps in standardizing the vocabulary and improving the accuracy of the sentiment analysis model. I will use NLTK's WordNetLemmatizer.


"""

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

# Download necessary NLTK data for lemmatization if not already present
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

try:
    nltk.data.find('corpora/omw-1.4') # Open Multilingual Wordnet
except LookupError:
    nltk.download('omw-1.4')

# Initialize WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    """
    Lemmatizes a list of tokens.
    """
    return [lemmatizer.lemmatize(word) for word in tokens]

# Apply the lemmatization function to the 'filtered_tokens' column
df_combined['lemmatized_tokens'] = df_combined['filtered_tokens'].apply(lemmatize_tokens)

# Display the first 5 rows with the new column
print("First 5 rows of df_combined with 'filtered_tokens' and 'lemmatized_tokens':")
print(df_combined[['filtered_tokens', 'lemmatized_tokens']].head())

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""**Reasoning**:
The preprocessing steps (cleaning, tokenization, stopword removal, and lemmatization) are now complete. For an LSTM-based sentiment analysis model, the text data needs to be converted into numerical sequences. This involves joining the lemmatized tokens back into a string and then using a tokenizer to transform these strings into sequences of integers, followed by padding to a uniform length. This prepares the data for input into the LSTM model.


"""

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Join lemmatized tokens back into a string for tokenizer input
df_combined['processed_text'] = df_combined['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))

# Parameters for tokenization and padding
VOCAB_SIZE = 10000  # Based on common practice for general text analysis
MAX_SEQUENCE_LENGTH = 100 # A reasonable length, can be adjusted based on distribution of message lengths

# Initialize and fit tokenizer
tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(df_combined['processed_text'])

# Convert text to sequences of integers
sequences = tokenizer.texts_to_sequences(df_combined['processed_text'])

# Pad sequences to ensure uniform length for LSTM input
padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')

# Add padded sequences to DataFrame (optional, but good for tracking)
df_combined['padded_sequences'] = list(padded_sequences)

print(f"Vocabulary size: {VOCAB_SIZE}")
print(f"Max sequence length: {MAX_SEQUENCE_LENGTH}")
print("Shape of padded sequences:", padded_sequences.shape)
print("First 5 padded sequences (first message):", padded_sequences[0][:10]) # Display first 10 tokens of first message

print("\nInformation about the combined DataFrame:")
df_combined.info()

"""## Define and Compile LSTM Sentiment Model

### Subtask:
Define the architecture of an LSTM-based sentiment analysis model and compile it. This model will be structured to process the padded numerical sequences generated from the chat messages.

**Reasoning**:
I need to define the LSTM model architecture, including an Embedding layer, an LSTM layer, and a Dense output layer, and then compile it with an appropriate optimizer, loss function, and metrics as specified in the instructions. This will involve using the `VOCAB_SIZE` and `MAX_SEQUENCE_LENGTH` from the previous preprocessing step.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define the LSTM model architecture
model = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_length=MAX_SEQUENCE_LENGTH),
    LSTM(128),
    Dense(3, activation='softmax') # Assuming 3 sentiment classes: positive, neutral, negative
])

# Compile the model
# Using 'sparse_categorical_crossentropy' as we assume integer labels for sentiment (0, 1, 2)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
print("LSTM Model Summary:")
model.summary()

"""**Reasoning**:
The model summary showed zero parameters because the model was not yet built. The `input_length` argument in the Embedding layer is also deprecated. To fix this, I will explicitly set `input_shape` in the Embedding layer to ensure the model is built correctly before calling `summary()`, and remove the deprecated `input_length` argument.


"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define the LSTM model architecture
model = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_shape=(MAX_SEQUENCE_LENGTH,)),
    LSTM(128),
    Dense(3, activation='softmax') # Assuming 3 sentiment classes: positive, neutral, negative
])

# Compile the model
# Using 'sparse_categorical_crossentropy' as we assume integer labels for sentiment (0, 1, 2)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
print("LSTM Model Summary:")
model.summary()

"""**Reasoning**:
The previous code successfully defined and compiled the LSTM model, and the model summary confirmed that parameters were correctly calculated, indicating the model was built as expected. The warning about `input_shape` is a recommendation from Keras and does not prevent the model from functioning correctly. Therefore, no further modifications are needed for this step.


"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define the LSTM model architecture
model = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_shape=(MAX_SEQUENCE_LENGTH,)),
    LSTM(128),
    Dense(3, activation='softmax') # Assuming 3 sentiment classes: positive, neutral, negative
])

# Compile the model
# Using 'sparse_categorical_crossentropy' as we assume integer labels for sentiment (0, 1, 2)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
print("LSTM Model Summary:")
model.summary()

"""**Reasoning**:
To enable sentiment analysis or training of the defined LSTM model, it is necessary to generate sentiment labels for each chat message. I will use NLTK's VADER sentiment intensity analyzer to assign a compound sentiment score to each message and then categorize these scores into 'positive', 'neutral', or 'negative' labels, which can serve as the target variable for the LSTM model.


"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download VADER lexicon if not already downloaded
try:
    nltk.data.find('sentiment/vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')

# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Function to get sentiment label from VADER compound score
def get_sentiment_label(text):
    if not isinstance(text, str) or not text.strip():
        return 'neutral' # Assign neutral to empty or non-string texts
    score = sia.polarity_scores(text)['compound']
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Apply sentiment analysis to the 'processed_text' column
df_combined['sentiment_vader'] = df_combined['processed_text'].apply(get_sentiment_label)

# Display the first few rows with the new sentiment column
print("First 5 rows of df_combined with 'processed_text' and 'sentiment_vader':")
print(df_combined[['processed_text', 'sentiment_vader']].head())

print("\nSentiment distribution:")
print(df_combined['sentiment_vader'].value_counts())

"""**Reasoning**:
The previous step successfully generated sentiment labels using VADER. Now, to prepare the data for training the LSTM model, I need to convert these categorical sentiment labels ('positive', 'neutral', 'negative') into numerical (integer) labels and then split the preprocessed data (padded sequences and numerical labels) into training and validation sets.


"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Map sentiment labels to numerical values
label_encoder = LabelEncoder()
df_combined['sentiment_label'] = label_encoder.fit_transform(df_combined['sentiment_vader'])

# Display the mapping
print("Sentiment Label Mapping:")
for i, label in enumerate(label_encoder.classes_):
    print(f"{label}: {i}")

# Prepare data for training
X = np.array(list(df_combined['padded_sequences']))
y = df_combined['sentiment_label'].values

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"\nShape of X_train: {X_train.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of y_val: {y_val.shape}")

# Display the first few sentiment labels and their numerical representation
print("\nFirst 5 sentiment labels and their numerical representation:")
print(df_combined[['sentiment_vader', 'sentiment_label']].head())

"""## Train the LSTM Model

### Subtask:
Train the defined LSTM model using the prepared training and validation sets.

**Reasoning**:
I need to train the LSTM model using the prepared training and validation datasets, `X_train`, `y_train`, `X_val`, and `y_val`. I will specify a reasonable number of epochs and a batch size, store the training history, and then print the keys of the history object to confirm the recorded metrics.
"""

history = model.fit(
    X_train,
    y_train,
    epochs=10,  # Using 10 epochs as a starting point
    batch_size=32, # Using a batch size of 32
    validation_data=(X_val, y_val)
)

print("\nTraining History Keys:")
print(history.history.keys())

"""## Predict sentiment with LSTM model

### Subtask:
Use the trained LSTM model to predict sentiment scores for all messages in the combined DataFrame and convert these predictions into categorical sentiment labels (e.g., 'positive', 'neutral', 'negative').

**Reasoning**:
I need to use the trained LSTM model to predict sentiment for all preprocessed messages, convert these predictions into categorical labels, and display their distribution along with a comparison to VADER's sentiment.
"""

import numpy as np

# Use the trained model to predict sentiment probabilities for all messages
lstm_predictions = model.predict(X)

# Convert probabilities to class labels (0, 1, or 2)
lstm_predicted_labels = np.argmax(lstm_predictions, axis=1)

# Inverse transform numerical class labels back to categorical sentiment labels
df_combined['sentiment_lstm'] = label_encoder.inverse_transform(lstm_predicted_labels)

print("LSTM Predicted Sentiment Distribution:")
print(df_combined['sentiment_lstm'].value_counts())

print("\nComparison of VADER and LSTM Sentiments (First 5 rows):")
print(df_combined[['processed_text', 'sentiment_vader', 'sentiment_lstm']].head())

"""## Analyze and Visualize Sentiment Trends with Streamlit

### Subtask:
Aggregate sentiment analysis results and create initial visualizations of sentiment distribution.

**Reasoning**:
To fulfill the subtask, I will import the necessary visualization libraries, calculate the sentiment distributions from both VADER and LSTM predictions, and then generate two bar plots to visually compare these distributions.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 2. Calculate the value counts for the sentiment columns
vader_sentiment_counts = df_combined['sentiment_vader'].value_counts()
lstm_sentiment_counts = df_combined['sentiment_lstm'].value_counts()

# Set a style for the plots
sns.set_style('whitegrid')

# 3. Create a bar plot for VADER's sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=vader_sentiment_counts.index, y=vader_sentiment_counts.values, palette='viridis')
plt.title('VADER Sentiment Distribution')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Messages')
plt.show()

# 4. Create a bar plot for the LSTM model's sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=lstm_sentiment_counts.index, y=lstm_sentiment_counts.values, palette='plasma')
plt.title('LSTM Sentiment Distribution')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Messages')
plt.show()

"""**Reasoning**:
The previous code generated `FutureWarning` messages because the `palette` argument was used without specifying `hue`. To resolve this and align with best practices in seaborn, I will explicitly set `hue` to the `x` variable and `legend` to `False` in both `sns.barplot` calls.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# 2. Calculate the value counts for the sentiment columns
vader_sentiment_counts = df_combined['sentiment_vader'].value_counts()
lstm_sentiment_counts = df_combined['sentiment_lstm'].value_counts()

# Set a style for the plots
sns.set_style('whitegrid')

# 3. Create a bar plot for VADER's sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=vader_sentiment_counts.index, y=vader_sentiment_counts.values, hue=vader_sentiment_counts.index, palette='viridis', legend=False)
plt.title('VADER Sentiment Distribution')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Messages')
plt.show()

# 4. Create a bar plot for the LSTM model's sentiment distribution
plt.figure(figsize=(10, 6))
sns.barplot(x=lstm_sentiment_counts.index, y=lstm_sentiment_counts.values, hue=lstm_sentiment_counts.index, palette='plasma', legend=False)
plt.title('LSTM Sentiment Distribution')
plt.xlabel('Sentiment Category')
plt.ylabel('Number of Messages')
plt.show()

"""## Develop Streamlit application with overall sentiment visualizations

### Subtask:
Develop a Streamlit application to display the overall sentiment distributions (VADER and LSTM) and a basic sentiment trend over time.

**Reasoning**:
To fulfill the subtask, I need to generate the Python code for a Streamlit application that displays the overall sentiment distributions from VADER and LSTM, along with a sentiment trend over time. This involves importing Streamlit and visualization libraries, then creating and displaying various plots.
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- Streamlit Application --- #

def run_streamlit_app(df_combined, label_encoder):
    st.title('WhatsApp Chat Sentiment Analysis Dashboard')

    st.write("### Data Overview")
    st.dataframe(df_combined.head())

    # --- Sentiment Distribution Visualizations ---
    st.write("### Sentiment Distribution (VADER vs. LSTM)")

    # VADER Sentiment Distribution
    vader_sentiment_counts = df_combined['sentiment_vader'].value_counts()
    fig_vader = plt.figure(figsize=(10, 6))
    sns.barplot(x=vader_sentiment_counts.index, y=vader_sentiment_counts.values, hue=vader_sentiment_counts.index, palette='viridis', legend=False)
    plt.title('VADER Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_vader)
    plt.close(fig_vader) # Close plot to free memory

    # LSTM Sentiment Distribution
    lstm_sentiment_counts = df_combined['sentiment_lstm'].value_counts()
    fig_lstm = plt.figure(figsize=(10, 6))
    sns.barplot(x=lstm_sentiment_counts.index, y=lstm_sentiment_counts.values, hue=lstm_sentiment_counts.index, palette='plasma', legend=False)
    plt.title('LSTM Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_lstm)
    plt.close(fig_lstm) # Close plot to free memory

    # --- Sentiment Trend Over Time ---
    st.write("### Sentiment Trend Over Time (LSTM)")

    # Ensure 'timestamp' is datetime and set as index
    df_trend = df_combined.set_index('timestamp').copy()

    # Convert LSTM sentiment labels to numerical for averaging
    # Use the existing label_encoder to ensure consistency
    df_trend['numerical_sentiment'] = label_encoder.transform(df_trend['sentiment_lstm'])

    # Resample daily and calculate mean sentiment
    daily_sentiment = df_trend['numerical_sentiment'].resample('D').mean().fillna(0) # Fill NaNs with 0 (neutral)

    fig_trend = plt.figure(figsize=(12, 7))
    sns.lineplot(x=daily_sentiment.index, y=daily_sentiment.values, marker='o')
    plt.title('Daily Average LSTM Sentiment Over Time')
    plt.xlabel('Date')
    plt.ylabel('Average Sentiment Score (0=Negative, 1=Neutral, 2=Positive)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig_trend)
    plt.close(fig_trend) # Close plot to free memory

# To run the Streamlit app locally, save the code above as a .py file (e.g., app.py)
# and execute `streamlit run app.py` in your terminal.
# Note: This environment does not support direct execution of Streamlit apps.
# The function `run_streamlit_app` is provided for completeness.

"""**Reasoning**:
The previous cell failed because the `streamlit` library was not installed. I need to install `streamlit` using `!pip install` before it can be imported and used.


"""

!pip install streamlit

"""**Reasoning**:
The Streamlit library has been successfully installed. Now I can proceed with generating the Streamlit application code. The previous code block already contains the full logic for the Streamlit app. I will re-execute that same code block.


"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- Streamlit Application --- #

def run_streamlit_app(df_combined, label_encoder):
    st.title('WhatsApp Chat Sentiment Analysis Dashboard')

    st.write("### Data Overview")
    st.dataframe(df_combined.head())

    # --- Sentiment Distribution Visualizations ---
    st.write("### Sentiment Distribution (VADER vs. LSTM)")

    # VADER Sentiment Distribution
    vader_sentiment_counts = df_combined['sentiment_vader'].value_counts()
    fig_vader = plt.figure(figsize=(10, 6))
    sns.barplot(x=vader_sentiment_counts.index, y=vader_sentiment_counts.values, hue=vader_sentiment_counts.index, palette='viridis', legend=False)
    plt.title('VADER Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_vader)
    plt.close(fig_vader) # Close plot to free memory

    # LSTM Sentiment Distribution
    lstm_sentiment_counts = df_combined['sentiment_lstm'].value_counts()
    fig_lstm = plt.figure(figsize=(10, 6))
    sns.barplot(x=lstm_sentiment_counts.index, y=lstm_sentiment_counts.values, hue=lstm_sentiment_counts.index, palette='plasma', legend=False)
    plt.title('LSTM Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_lstm)
    plt.close(fig_lstm) # Close plot to free memory

    # --- Sentiment Trend Over Time ---
    st.write("### Sentiment Trend Over Time (LSTM)")

    # Ensure 'timestamp' is datetime and set as index
    df_trend = df_combined.set_index('timestamp').copy()

    # Convert LSTM sentiment labels to numerical for averaging
    # Use the existing label_encoder to ensure consistency
    df_trend['numerical_sentiment'] = label_encoder.transform(df_trend['sentiment_lstm'])

    # Resample daily and calculate mean sentiment
    daily_sentiment = df_trend['numerical_sentiment'].resample('D').mean().fillna(0) # Fill NaNs with 0 (neutral)

    fig_trend = plt.figure(figsize=(12, 7))
    sns.lineplot(x=daily_sentiment.index, y=daily_sentiment.values, marker='o')
    plt.title('Daily Average LSTM Sentiment Over Time')
    plt.xlabel('Date')
    plt.ylabel('Average Sentiment Score (0=Negative, 1=Neutral, 2=Positive)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    st.pyplot(fig_trend)
    plt.close(fig_trend) # Close plot to free memory

# To run the Streamlit app locally, save the code above as a .py file (e.g., app.py)
# and execute `streamlit run app.py` in your terminal.
# Note: This environment does not support direct execution of Streamlit apps.
# The function `run_streamlit_app` is provided for completeness.

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# It's good practice to encapsulate the application logic within a function
# so it can be called with the necessary dataframes and encoders.
# For Streamlit, the main script should contain the UI elements directly.

# Assuming df_combined and label_encoder are available in the global scope
# or passed to the app function if it were designed differently.
# For this demonstration, we'll use the globally defined variables from the notebook state.

st.set_page_config(layout="wide")

st.title('WhatsApp Chat Sentiment Analysis Dashboard')

st.write("### Data Overview")
st.dataframe(df_combined.head())

# --- Sentiment Distribution Visualizations ---
st.write("### Sentiment Distribution (VADER vs. LSTM)")

col1, col2 = st.columns(2)

with col1:
    # VADER Sentiment Distribution
    vader_sentiment_counts = df_combined['sentiment_vader'].value_counts()
    fig_vader = plt.figure(figsize=(8, 5))
    sns.barplot(x=vader_sentiment_counts.index, y=vader_sentiment_counts.values, hue=vader_sentiment_counts.index, palette='viridis', legend=False)
    plt.title('VADER Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_vader)
    plt.close(fig_vader) # Close plot to free memory

with col2:
    # LSTM Sentiment Distribution
    lstm_sentiment_counts = df_combined['sentiment_lstm'].value_counts()
    fig_lstm = plt.figure(figsize=(8, 5))
    sns.barplot(x=lstm_sentiment_counts.index, y=lstm_sentiment_counts.values, hue=lstm_sentiment_counts.index, palette='plasma', legend=False)
    plt.title('LSTM Sentiment Distribution')
    plt.xlabel('Sentiment Category')
    plt.ylabel('Number of Messages')
    st.pyplot(fig_lstm)
    plt.close(fig_lstm) # Close plot to free memory

# --- Sentiment Trend Over Time ---
st.write("### Sentiment Trend Over Time (LSTM)")

# Ensure 'timestamp' is datetime and set as index for resampling
df_trend = df_combined.copy()
df_trend['timestamp'] = pd.to_datetime(df_trend['timestamp'])
df_trend = df_trend.set_index('timestamp')

# Convert LSTM sentiment labels to numerical for averaging
# Use the existing label_encoder to ensure consistency
df_trend['numerical_sentiment'] = label_encoder.transform(df_trend['sentiment_lstm'])

# Resample daily and calculate mean sentiment
daily_sentiment = df_trend['numerical_sentiment'].resample('D').mean().fillna(0) # Fill NaNs with 0 (neutral)

fig_trend = plt.figure(figsize=(15, 7))
sns.lineplot(x=daily_sentiment.index, y=daily_sentiment.values, marker='o')
plt.title('Daily Average LSTM Sentiment Over Time')
plt.xlabel('Date')
plt.ylabel('Average Sentiment Score (0=Negative, 1=Neutral, 2=Positive)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
st.pyplot(fig_trend)
plt.close(fig_trend) # Close plot to free memory

st.write("**Instructions to Run the Streamlit App Locally:**")
st.markdown("""
1.  **Save the code**: Copy the code from this cell and save it as a Python file (e.g., `app.py`) on your local machine.
2.  **Install Streamlit**: If you haven't already, install Streamlit by running `pip install streamlit` in your terminal.
3.  **Run the app**: Navigate to the directory where you saved `app.py` in your terminal and run `streamlit run app.py`.

The app will open in your web browser, displaying the sentiment analysis dashboard.
""")

"""## Final Task

### Subtask:
Present the findings from the emotional content analysis, the Streamlit application code, and instructions on how to run it.

## Summary:

### Data Analysis Key Findings

*   **Successful Data Parsing**: The WhatsApp chat data from two files (`/content/WhatsApp Chat with ECE A 5th semester.txt` and `/content/WhatsApp Chat with HackNx Community.txt`) was successfully loaded and parsed after multiple refinements to the regular expression and timestamp parsing logic. The parsing yielded 1,667 messages from the ECE chat and 387 messages from the HackNx chat, totaling 2,054 messages in the combined DataFrame.
*   **Comprehensive Text Preprocessing**: Messages underwent a robust cleaning process including lowercasing, removal of URLs, specific placeholders (`<Media omitted>`), non-alphanumeric characters, tokenization, stopword removal, and lemmatization. Finally, these processed texts were converted into padded numerical sequences of uniform length (100) using a Keras `Tokenizer` with a vocabulary size of 10,000, preparing them for the LSTM model.
*   **LSTM Model Architecture and Data Preparation**: An LSTM-based sentiment analysis model was successfully defined and compiled. It includes an `Embedding` layer (1,280,000 parameters), an `LSTM` layer (131,584 parameters), and a `Dense` output layer for 3 sentiment classes (387 parameters), totaling 1,411,971 trainable parameters. NLTK's VADER was used to assign initial sentiment labels ('positive', 'neutral', 'negative') to messages, with 1,476 neutral, 494 positive, and 84 negative messages. These labels were then numerically encoded and split into training and validation sets for model training.
*   **Model Training and Performance**: The LSTM model was trained for 10 epochs with a batch size of 32. The training history showed a decrease in both training and validation loss, and an increase in training and validation accuracy over the epochs, indicating successful learning.
*   **LSTM Sentiment Predictions**: The trained LSTM model was used to predict sentiments for all messages. The distribution of LSTM-predicted sentiments was 1,482 neutral and 572 positive. Notably, the LSTM model did not predict any 'negative' sentiments for the given dataset, contrasting with VADER's predictions.
*   **Initial Visualizations and Streamlit App Development**: Bar plots illustrating the sentiment distributions for both VADER and LSTM predictions were successfully generated. The Python code for a Streamlit application was developed, which includes displaying raw data, these sentiment distribution plots, and a daily average sentiment trend over time (based on LSTM predictions).

### Insights or Next Steps

*   **Investigate LSTM's Absence of Negative Predictions**: A key insight is the significant difference in sentiment distribution between VADER and the LSTM model, particularly the LSTM's complete lack of 'negative' classifications. This warrants further investigation into the LSTM model's sensitivity, the training data's characteristics, or potential imbalance in the sentiment labels during training.
*   **Enhance Streamlit Interactivity and Analysis**: The developed Streamlit application provides a solid foundation. Next steps could involve adding interactive filters (e.g., by sender, time range), more granular temporal analysis (e.g., weekly, monthly), and a comparison view of VADER vs. LSTM sentiment per message to provide deeper insights into the emotional dynamics of the chat data.
"""